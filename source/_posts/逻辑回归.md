---
title: 逻辑回归
categories: 机器学习
comments: true
mathjax: true
tags:
  - 分类
  - Scikit-Learn
abbrlink: 60504
date: 2018-02-17 19:56:12
---

逻辑回归（ Logistic Regression ）常用来预测属于某一类别的概率。如果概率大于 50% 那么就认为它属于该类别，标记为 1 ，否则就是标记为 0 。这也就形成了二分类问题。

<!--more-->

## 预测概率

和 [线性回归](/posts/52662/) 类似，逻辑回归也是计算输入特征的加权求和，但是不同点是，逻辑回归输入是属于某类别的概率。

$$
\hat p = h\_\theta \left(X\right) = \sigma \left(\theta ^T \cdot X\right)
$$

其中 sigmoid 函数表达式和图像如下：

$$
\sigma \left(t\right) = \frac{1}{1+e^{-t}}
$$

![](sigmoid.svg "Sigmoid 函数")

一旦计算出概率 $\hat p$ ，就可以判断是不是属于某类别。

$$
\hat y =
\begin{cases}
0,  & \text{$\hat p \lt 0.5$}, \\\\
1, & \text{$\hat p \geq 0.5$;}
\end{cases}
$$

因为当 $t \lt 0$ 时 $\sigma \left(t\right) \lt 0.5$ ，当 $t \geq 0$ 时 $\sigma \left(t\right) \geq 0.5$ ，所以如果 $\theta ^T\cdot X$ 为正数，那么就预测结果为 1 。

## 训练和损失函数

损失函数使用 $-\log \left(t\right)$ ，因为当 $t$ 接近 0 的时候，损失将非常大；当 $t$ 接近 1 的时候，损失接近 0 。

![-log(t) 函数](log.svg)

损失函数表达式：

$$
c \left(\theta \right) =
\begin{cases}
\- \log \left(\hat p\right),  & \text{if $y$ = 1}, \\\\
\- \log \left(1 - \hat p\right), & \text{if $y$ = 0}.
\end{cases}
$$

可以得到整个训练集的平均损失函数 $J\left(\theta \right)$ ，表达式如下：

$$
J\left(\theta \right) = \- \frac{1}{m}\sum \_{i=1}^m \left[y^{\left(i\right)}\log \left(\hat p^{\left(i\right)}\right) + \left( 1 \- y^{\left(i\right)}\right)\log \left(1 \- \hat p^{\left(i\right)}\right)\right]
$$

这里注意系数，乘上 $y$ 和 $1 - y$ 正好可以得到预测成功和失败的损失函数。在这里求最小损失函数没有线性回归中的闭式解 Normal Equation 。但是损失函数是可导的，所以可以利用梯度下降来求最优解。

$$
\begin{equation}\begin{split}
\frac{\partial}{\partial \theta \_j}J\left(\theta \right) &= \frac{1}{m}\sum \_{i=1}^m\left[\left(1 \- y^{\left(i\right)} \right)\frac{1}{1 \- \hat p^{\left(i\right)}} \- y^{\left(i\right)}\frac{1}{\hat p^{\left(i\right)}} \right]\frac{\partial}{\partial \theta \_j}\hat p^{\left(i\right)}\\\\
&=\frac{1}{m}\sum \_{i=1}^m\left[\left(1 \- y^{\left(i\right)} \right)\frac{1}{1 \- \hat p^{\left(i\right)}} \- y^{\left(i\right)}\frac{1}{\hat p^{\left(i\right)}} \right]\hat p^{\left(i\right)}\left(1 \- \hat p^{\left(i\right)}\right)\frac{\partial}{\partial \theta \_j}\left(\theta ^T \cdot X\right)\\\\
&=\frac{1}{m}\sum \_{i=1}^m\left[\left(1 \- y^{\left(i\right)} \right)\hat p^{\left(i\right)} \- y^{\left(i\right)}\left(1 \- \hat p^{\left(i\right)} \right)\right]x\_j\\\\
&=\frac{1}{m}\sum \_{i=1}^m\left(\sigma \left(\theta ^T\cdot x^{\left(i\right)}\right) \- y^{\left(i\right)}\right)x\_j^{\left(i\right)}
\end{split}\end{equation}
$$

## 决策边界

```python
from sklearn import datasets
import numpy as np
from sklearn.linear_model import LogisticRegression
iris = datasets.load_iris()
x = iris["data"][:, 3:]
y = (iris["target"] == 2).astype(np.int)
log_reg = LogisticRegression()
log_reg.fit(x, y)
x_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_predict = log_reg.predict_proba(x_new)
plt.plot(x_new, y_predict[:, 1], "g--", label="Iris-Virginica")
plt.plot(x_new, y_predict[:, 0], "b--", label="Not Iris-Virginica")
```

![决策边界](output.svg "决策边界")

从图中可以看到决策边界为 1.6cm ，因为在此处预测的概率为 50% ，大于 1.6cm 就会被预测为 Iris- Virginica ，小于 1.6cm 则被预测为 Not Iris-Virginica 。  
逻辑回归模型和其他线性回归模型一样，也可以使用 $L\_1$ 或者 $L\_2$ 范数进行正则化，在 Scikit-Learn 中默认使用 $L\_2$ 正则化系数。

