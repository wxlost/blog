---
title: 线性回归
categories: 机器学习
comments: true
mathjax: true
tags:
  - 回归
  - Scikit-Learn
abbrlink: 52662
date: 2018-02-11 22:10:43
---

线性回归是机器学习中最简单的模型，线性回归算法属于回归算法，是监督学习方法。sklearn 当然也有线性回归，通过一个小例子实践一下。

<!--more-->

## 表达式

线性回归模型的预测值的表达式如下：

$$
\hat y = \theta \_0 + \theta \_1x\_1 + \theta \_2x\_2 + \dots + \theta \_nx\_n
$$

- $\hat y$ 代表预测值
- $n$ 代表了模型有 $n$ 个特征值
- $x\_i$ 是第 $i$ 个特征的值
- $\theta \_j$ 是模型中第 $j$ 个参数

把上面的表达式写成向量形式：

$$
\hat y = h\_\theta \left(x\right) = \theta ^T \cdot x
$$

## 损失函数

在线性回归模型的训练中，最常用的损失函数就是均方误差 (mean-square error, MSE) 。均方误差表达式如下：

$$
MSE\left(X,h\_\theta \right) = \frac{1}{m}\sum\_{i=1}^m\left(\theta ^T\cdot x^{\left(i\right)}\right)^2
$$

需要找出 $\theta$ 使得损失函数的值最小，有一个闭式解可以求出 $\theta$ 。所谓[闭式解](https://en.wikipedia.org/wiki/Closed-form_expression)就是代入值就可以得到结果，而不需要迭代的到近似值。可以使用 Normal Equation 来求出 $\theta$ 。

$$
\hat \theta = \left(X^T\cdot X\right)^{-1}\cdot X^T\cdot y
$$

- $\hat \theta$ 就是使得损失函数的值最小的 $\theta$
- y 是输出的目标值 $y^{\left(1\right)}$ 到 $y^{\left(m\right)}$

## 实践

生成 $y = 4 + 3x$ 加上噪声的数据。

```python
import numpy as np
x = 2 * np.random.rand(100, 1)
y = 4 + 3 * x + np.random.rand(100, 1)
```

![](randomly-generated-linear-dataset.svg "数据可视化")

```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x, y)
x_new = np.array([[2]])
lin_reg.predict(x_new)    # array([[ 10.52802502]])
```

![](Linear-Regression-model-predictions.svg "预测结果")